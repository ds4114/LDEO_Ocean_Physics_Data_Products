{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_setup(extra_packages=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Function created to simplify repeated code between data ingestion and processing steps\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    extra_packages: bool \n",
    "        Defines if several additional packages should be loaded\n",
    "    verbose: bool\n",
    "        Set to true to see additional print statements on variables and progress\n",
    "    \"\"\"\n",
    "    \n",
    "    global np; import numpy as np\n",
    "    global os; import os\n",
    "    global yaml; import yaml\n",
    "    global xr; import xarray as xr\n",
    "    global pd; import pandas as pd\n",
    "    global datetime; import datetime\n",
    "    global plt; import matplotlib.pyplot as plt\n",
    "    if verbose: print(\"Imported standard packages: [numpy, os, yaml, xarray, pandas, datetime, pyplot]\")\n",
    "    \n",
    "    #This file contains configuration details like API keys and passwords\n",
    "    global global_vars; global_vars = yaml.safe_load(open('../../config.yml', 'r') )\n",
    "    if verbose: print(\"Loaded custom repository configuration file\")\n",
    "    \n",
    "    if extra_packages:\n",
    "        global ZipFile;  from zipfile import ZipFile\n",
    "        global ftplib;   import ftplib \n",
    "        global cdsapi;   import cdsapi\n",
    "        global requests; import requests\n",
    "        global glob;     import glob\n",
    "        global ma;       import numpy.ma as ma\n",
    "        global gzip;     import gzip\n",
    "        #There are some issues with xesmf installation so we need to point to a file required for the package. See README.md for details.\n",
    "        ESMFMKFILE = global_vars['ESMFMKFILE']\n",
    "        %env ESMFMKFILE $ESMFMKFILE  \n",
    "        global xesmf; import xesmf\n",
    "        if verbose: print(\"Imported other packages: [xesmf, zipfile, ftplib, cdsapi, requests, numpy-ma, glob, gzip]\")\n",
    "\n",
    "    \n",
    "    #This variable sets the output filetype for SSS and CHL data and needs to specified explicitly because of the unique way the are downloaded.\n",
    "    #By default, all data source downloads will default to the netcdf format unless using cloud storage. \n",
    "    #When using cloud storage, it is recommended to use ARCO (Analysis-Ready Cloud-Optimized) formats like Zarr over NetCDF\n",
    "    global output_file_type; output_file_type = '.zarr' if global_vars['download_folder'][0:5] == 'gs://' else '.nc'\n",
    "    print(f'Files will be outputed as: {output_file_type}')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cds_api_key():\n",
    "    \"\"\"\n",
    "    Some data (SLP, SST) requires an account on European Centre for Medium-Range Weather Forecasts and an API key\n",
    "    More info can be found https://cds.climate.copernicus.eu/api-how-to\n",
    "    To use the API package (conda install -c conda-forge cdsapi), we need to next install the API key using this function\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    cds_url= \"url: https://cds.climate.copernicus.eu/api/v2\"\n",
    "    cds_key= 'key: '+ global_vars['cds_api_key']  #from the configuration file\n",
    "    file = os.path.expanduser('~')+'/.cdsapirc'\n",
    "    if not (os.path.isfile(file)): \n",
    "        cds_file = open(file, \"w\")\n",
    "        cds_file.write(cds_url+'\\n')\n",
    "        cds_file.write(cds_key)\n",
    "        cds_file.close()\n",
    "        print(\"Installed ECMWF CDS API key\")\n",
    "    else:\n",
    "        print(\"ECMWF CDS API key already installed\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ingestion_prep(start_yearmonth='1979-01', end_yearmonth='2022-12', verbose = False):\n",
    "    \"\"\"\n",
    "    Function created to simplify repeated code when setting variables during data ingestion and processing steps\n",
    "    See function comments or set verbose=True to see variables\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    import xarray as xr\n",
    "    \n",
    "    generate_cds_api_key()\n",
    "    \n",
    "    global today_yearmonth; today_yearmonth = datetime.datetime.now().strftime('%Y%m')\n",
    "    global acquisition_start_year; acquisition_start_year = int(start_yearmonth[0:4])\n",
    "    global acquisition_end_year;   acquisition_end_year = int(end_yearmonth[0:4])\n",
    "    \n",
    "    global processed_start_yearmonth; processed_start_yearmonth = '1982-01'  \n",
    "    global processed_end_yearmonth; processed_end_yearmonth = '2022-12'  \n",
    "    global processed_start_yearmonth_back_in_time; processed_start_yearmonth_back_in_time = '1959-01'  \n",
    "    \n",
    "    global figsizew; figsizew = 6\n",
    "    global figsizeh; figsizeh = 3  \n",
    "    \n",
    "    #create desired resolution and time frame\n",
    "    global ylat; ylat = xr.DataArray(data=[x+.5 for x in range(-90, 90, 1)], dims=['ylat'], coords=dict( ylat=(['ylat'],[x+.5 for x in range(-90, 90, 1)]) ),)\n",
    "    global xlon; xlon = xr.DataArray(data=[x+.5 for x in range(-180,180,1)], dims=['xlon'], coords=dict( xlon=(['xlon'],[x+.5 for x in range(-180,180,1)]) ),)\n",
    "    global ttime; ttime = pd.date_range(start=str(processed_start_yearmonth), end=str(processed_end_yearmonth),freq='MS') + np.timedelta64(14, 'D') #time should be monthly on the middle of the month\n",
    "            #note that the time doesnt affect regridding but we do use this time to overwrite the monthly dates so its consistent\n",
    "    global ttime_back_in_time; ttime_back_in_time = pd.date_range(start=str(processed_start_yearmonth_back_in_time), end=str(processed_end_yearmonth),freq='MS') + np.timedelta64(14, 'D')  #for back in time portion\n",
    "\n",
    "    global ideal_grid; ideal_grid = xr.Dataset({'time':(['time'],ttime.values), 'latitude':(['latitude'],ylat.values),'longitude':(['longitude'],xlon.values)}) #must be named this way for old XESFM versions\n",
    "    global ideal_grid_back_in_time; ideal_grid_back_in_time = xr.Dataset({'time':(['time'],ttime_back_in_time.values), 'latitude':(['latitude'],ylat.values),'longitude':(['longitude'],xlon.values)}) \n",
    "    \n",
    "    ##################################\n",
    "    if verbose:\n",
    "        print(\"\"\"\n",
    "#The following two variables are used to acquire select data when it is uploaded by month or year.\n",
    "    #Data that is not uploaded by year includes: SST (NOAA); MLD (deBoyer & Argo), fCO2 (SOCAT), xCO2, Coastal, SeaFlux\n",
    "#These set the start and end years (inclusive) and do not need to be changed.\n",
    "#Some years/months of data may not available (because prior to when data was gathered or too recent for the source).\n",
    "    #In those cases any available data is obtained in this range. Specifically, \n",
    "        #SST (NOAA) data only 1981-present\n",
    "        #SST (ERA5) data only 1940-present\n",
    "        #SST (JRA55) data only 1958-2023\n",
    "        #SSS data only 1900-present\n",
    "        #MLD (deBoyer and Argo) data only an averaged 12 months\n",
    "        #CHL data only 1997-present\n",
    "        #fCO2 data only 1970-2022\n",
    "        #SLP data only 1940-2022\n",
    "        #xCO2 data only 1979-present\n",
    "        #Coastal data only an averaged 12 months \n",
    "        #SeaFlux data only 1982-2022\n",
    "    #These limitations are hardcoded so other sources or links would be needed to download outside of this range \n",
    "         \"\"\")\n",
    "        print(f'acquisition_start_year: {acquisition_start_year}')\n",
    "        print(f'acquisition_end_year: {acquisition_end_year}')\n",
    "        \n",
    "        print(\"\"\"\n",
    "# The following three variables are used to slice the data to desired time frames for consistency or to backfill historical data with averages. No changes required.\n",
    "    # Given the time range limitiations of the raw data, this primarily affects MLD (1 year repeated) and CHL (linear interpoloation). \n",
    "    # For other data like SLP, this limits the real-time (experimental) data from recent months\n",
    "    # The Back_In_Time variable is used for some products and variables to extend beyond the standard starting timeframe\n",
    "    # Note that all input files should have data in at least this time frame or you may encounter some errors\n",
    "    \"\"\")\n",
    "        print(f'processed_start_yearmonth: {processed_start_yearmonth}')\n",
    "        print(f'processed_end_yearmonth: {processed_end_yearmonth}') \n",
    "        print(f'processed_end_yearmonth: {processed_start_yearmonth_back_in_time}') \n",
    "    \n",
    "        print(f'Figure size for maps (width/height): {figsizew}, {figsizeh}')\n",
    "    \n",
    "        print(f'Additional derived variables not listed. See function for more details.')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, dest_folder: str, dest_file: str, overwrite=False, create_dest=False):\n",
    "    \"\"\"\n",
    "    Wrapper function to download data from a URL and save it to a folder. If the destination path is Google Cloud Storage and the\n",
    "    source data is a netcdf, the downloaded data will be saved as a .zarr file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        The URL to the direct download\n",
    "    dest_folder: string\n",
    "        Desired file path for the destination folder\n",
    "    dest_file: string\n",
    "        Desired file name for the downloaded file\n",
    "    overwrite: boolean\n",
    "        if true, will overwrite the dest_file at the dest_folder location\n",
    "    create_dest: boolean\n",
    "        if true, the destination folder will be created automatically;\n",
    "        otherwise, the function will stop and require a user to manually create the folder as an extra validation step\n",
    "    \"\"\"\n",
    "    \n",
    "    if dest_folder[0:5] == 'gs://':\n",
    "        if url[-3:] == '.nc':\n",
    "            download_to_gs_as_zarr(url, dest_folder, dest_file, overwrite, create_dest)\n",
    "        elif url[-4:] == '.csv':\n",
    "            print(\"Attempting to transfer non-ARCO file extension to cloud. This function is in-progress and may not work for large files.\")\n",
    "            tmp_folder = '../../tmp/'\n",
    "            download_to_folder(url, tmp_folder, dest_file, overwrite, create_dest=True)\n",
    "            import gcsfs\n",
    "            fs = gcsfs.GCSFileSystem()\n",
    "            fs.touch(dest_folder) #create empty file to organize folder structure\n",
    "            fs.put(tmp_folder+dest_file, dest_folder+dest_file, recursive=True)\n",
    "            print(f'Moved to GS {dest_folder+dest_file}.')\n",
    "            os.remove(tmp_folder+dest_file)\n",
    "        else:\n",
    "            print('Cancelling - This cloud storage function currently only supports transfer of netcdf files; please confirm the download file extension.')\n",
    "            return None\n",
    "    else:\n",
    "        download_to_folder(url, dest_folder, dest_file, overwrite, create_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some modification may be needed for additional operating systems\n",
    "def download_to_folder(url: str, dest_folder: str, dest_file: str, overwrite=False, create_dest=False):\n",
    "    \"\"\"\n",
    "    Downloads data from a URL and saves it to a folder\n",
    "    Modified from https://stackoverflow.com/questions/56950987/download-file-from-url-and-save-it-in-a-folder-python\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        The URL to the direct download\n",
    "    dest_folder: string\n",
    "        File path for the destination folder\n",
    "    dest_file: string\n",
    "        File name for the downloaded file\n",
    "    overwrite: boolean\n",
    "        if true, will overwrite the dest_file at the dest_folder location\n",
    "    create_dest: boolean\n",
    "        if true, the destination folder will be created automatically;\n",
    "        otherwise, the function will stop and require a user to manually create the folder as an extra validation step\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import requests\n",
    "    \n",
    "    if os.path.exists(dest_folder):\n",
    "        pass\n",
    "    else:\n",
    "        if create_dest:\n",
    "            os.makedirs(dest_folder)  # create folder if it does not exist\n",
    "        else:\n",
    "            print(f\"Please confirm the destination folder exists: {dest_folder}. Or set create_dest=True.\")  #extra check to place data in correct spot\n",
    "            return None\n",
    "\n",
    "    file_path = os.path.join(dest_folder, dest_file)\n",
    "    \n",
    "    if overwrite or not (os.path.isfile(file_path)):   #if you want to overwrite or if the file doesnt already exists, then download\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.ok:\n",
    "            print(f\"Saving {url} to {file_path}...\")\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024 * 1024 * 10):  #10 MB chunk size; could increase for faster download speed\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        f.flush()\n",
    "                        os.fsync(f.fileno())\n",
    "            print(\"Complete\")\n",
    "        else:  # HTTP status code 4XX/5XX. This could be incorporated into a try/catch to handle separately\n",
    "            print(f\"Download failed: status code {r.status_code}\\n{r.text}\")\n",
    "            print(url)\n",
    "    else:\n",
    "        print(f\"File {dest_file} already exists at {dest_folder} - (skipping download from {url} )\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some modification may be needed for additional operating systems\n",
    "def download_to_gs_as_zarr(url: str, dest_folder: str, dest_file: str, overwrite=False, create_dest=False):\n",
    "    \"\"\"\n",
    "    Downloads NetCDF data from a URL and saves it to a temporary folder; then loads and copies it to destination Google Storage as a zarr file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        The URL to the direct download\n",
    "    dest_folder: string\n",
    "        File path for the destination folder\n",
    "    dest_file: string\n",
    "        File name for the downloaded file\n",
    "    overwrite: boolean\n",
    "        if true, will overwrite the dest_file at the dest_folder location\n",
    "    create_dest: boolean\n",
    "        if true, the destination folder will be created automatically;\n",
    "        otherwise, the function will stop and require a user to manually create the folder as an extra validation step\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import requests\n",
    "    import gcsfs\n",
    "    import xarray as xr\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    file_path = os.path.join(dest_folder, dest_file)\n",
    "    zarr_dest_file = dest_file.replace('.nc','.zarr')  #hardcoded for netcdfs\n",
    "    zarr_file_path = file_path.replace('.nc','.zarr')\n",
    "    tmp_file_path = '../../tmp/'+dest_file\n",
    "    \n",
    "    #check if destination path exists\n",
    "    if fs.exists(dest_folder):\n",
    "        pass\n",
    "    else:\n",
    "        if create_dest:\n",
    "            print(f\"Creating destination folder: {dest_folder}...\")  \n",
    "            fs.touch(dest_folder) #create empty file to organize folder structure\n",
    "        else:\n",
    "            print(f\"Please confirm the destination folder exists using the touch() function: {dest_folder}. Or set create_dest=True.\")  #extra check to place data in correct spot\n",
    "            return None\n",
    "    \n",
    "    #check if file already exists in destination\n",
    "    if fs.exists(zarr_file_path) and (not overwrite):\n",
    "        #TODO - the exists() function seems unstable; if you remove files from another kernels the result may not register\n",
    "        print(f\"File {zarr_dest_file} already exists at {dest_folder} - (skipping download from {url} )\")\n",
    "        return None\n",
    "    \n",
    "    #download to temp folder\n",
    "    print(f\"Downloading to ../../tmp/...\")    \n",
    "    download_to_folder(url, '../../tmp/', dest_file, overwrite=overwrite, create_dest=True)\n",
    "    \n",
    "    #copy to destination\n",
    "    tmp_xr = xr.open_dataset(tmp_file_path)\n",
    "    print(f\"Transferring to GS {zarr_file_path}...\")\n",
    "    tmp_xr.to_zarr(zarr_file_path, mode='w')\n",
    "    \n",
    "    #remove temp file\n",
    "    os.remove(tmp_file_path)\n",
    "    print(\"Complete\")\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdsapi_custom_download(year: int,  months: list, variable: str, dest_folder: str, dest_file: str\n",
    "                           ,tmp_folder='../../tmp/'\n",
    "                          ,overwrite=False, create_dest=False ):\n",
    "    \"\"\"\n",
    "    Downloads data using the cdsapi (European Centre for Medium-Range Weather Forecasts)\n",
    "    If the destination folder is cloud storage, the function saves the file to a temporary folder; then loads and copies it to destination Google Storage as a zarr file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year: int\n",
    "        The year of the data to be downloaded\n",
    "    months:\n",
    "        A list of months (set of integers from 1 to 12) of the data to be downloaded\n",
    "    dest_folder: string\n",
    "        Local file path for the destination folder\n",
    "    dest_file: string\n",
    "        Local file name for the downloaded file\n",
    "    overwrite: boolean\n",
    "        if true, will overwrite the dest_file at the dest_folder location\n",
    "    create_dest: boolean\n",
    "        if true, the destination folder will be created automatically;\n",
    "        otherwise, the function will stop and require a user to manually create the folder as an extra validation step\n",
    "    \"\"\"\n",
    "    import cdsapi\n",
    "    import os\n",
    "    import requests\n",
    "    import xarray as xr\n",
    "    import gcsfs\n",
    "    \n",
    "    file_path = os.path.join(dest_folder, dest_file)\n",
    "    tmp_file_path = tmp_folder+dest_file\n",
    "    gfs = False\n",
    "    if dest_folder[0:5] == 'gs://':\n",
    "        gfs = True\n",
    "        fs = gcsfs.GCSFileSystem()\n",
    "        zarr_dest_file = dest_file.replace('.nc','.zarr')  #hardcoded for netcdfs\n",
    "        zarr_file_path = file_path.replace('.nc','.zarr')\n",
    "    \n",
    "    #check if destination path exists\n",
    "    if gfs:\n",
    "        if not (fs.exists(dest_folder)):\n",
    "            if create_dest:   #if dest folder doesnt exist but we want to create it\n",
    "                print(f\"Creating destination folder: {dest_folder}...\")  \n",
    "                fs.touch(dest_folder) #create empty file to organize folder structure\n",
    "            else:\n",
    "                print(f\"Please confirm the destination folder exists using the touch() function: {dest_folder}. Or set create_dest=True.\")  #extra check to place data in correct spot\n",
    "                return None\n",
    "    else:\n",
    "        if not (os.path.exists(dest_folder)):\n",
    "            if create_dest:\n",
    "                os.makedirs(dest_folder)  \n",
    "            else:\n",
    "                print(f\"Please confirm the destination folder exists: {dest_folder}. Or set create_dest=True.\")  #extra check to place data in correct spot\n",
    "                return None\n",
    "    \n",
    "    #check if data already exists or was previously downloaded\n",
    "    if gfs:\n",
    "        if fs.exists(zarr_file_path) and (not overwrite):\n",
    "            print(f\"File {zarr_dest_file} already exists - (skipping download for {year} )\")\n",
    "            return None\n",
    "    else:\n",
    "        if os.path.isfile(file_path) and (not overwrite):  \n",
    "            print(f\"File {dest_file} already exists - (skipping download from {year} )\")\n",
    "            return None\n",
    "    \n",
    "    #now download to temp folder\n",
    "    if not os.path.exists(tmp_folder): os.makedirs(tmp_folder)\n",
    "    #print(f\"Downloading to \"+ tmp_folder)    \n",
    "    \n",
    "    c = cdsapi.Client()\n",
    "    if variable == 'satellite-sea-level-global':   #newly added 12/20/23\n",
    "         c.retrieve(\n",
    "            'satellite-sea-level-global',\n",
    "            {\n",
    "                'format': 'zip', #netcdf not available for this one; can extract in another function/step\n",
    "                'year': year,\n",
    "                'variable': 'monthly_mean',  #confusingly the 'variable' here is the mean rather than the product_type but w/e\n",
    "                'month': months,\n",
    "                'version': 'vDT2021'\n",
    "            },\n",
    "            tmp_file_path)\n",
    "    else:\n",
    "        product_type = 'monthly_averaged_reanalysis'\n",
    "        #if variable == ['10m_u_component_of_wind', '10m_v_component_of_wind']:\n",
    "        #    product_type = 'monthly_averaged_reanalysis_by_hour_of_day'\n",
    "        c.retrieve(\n",
    "            'reanalysis-era5-single-levels-monthly-means',\n",
    "            {\n",
    "                'format': 'netcdf',\n",
    "                'year': year,\n",
    "                'variable': variable,\n",
    "                'product_type': product_type,\n",
    "                'month': months,\n",
    "                #'time': '00:00'\n",
    "                 'time': [\n",
    "                '00:00', '01:00', '02:00',\n",
    "                '03:00', '04:00', '05:00',\n",
    "                '06:00', '07:00', '08:00',\n",
    "                '09:00', '10:00', '11:00',\n",
    "                '12:00', '13:00', '14:00',\n",
    "                '15:00', '16:00', '17:00',\n",
    "                '18:00', '19:00', '20:00',\n",
    "                '21:00', '22:00', '23:00',\n",
    "                 ]\n",
    "            },\n",
    "            tmp_file_path)\n",
    "    \n",
    "    \n",
    "    #this next part is kind of ugly but Im just trying to handle zip files, zarr files, and netcdfs\n",
    "    if variable == 'satellite-sea-level-global':\n",
    "        #after downloading, extract, then transfer all to destination\n",
    "        ZipFile(tmp_file_path).extractall(tmp_folder)\n",
    "        files = glob.glob(tmp_folder+'dt_global_twosat_phy_l4_*.nc') #hardcoded\n",
    "        for f in files:  #for each file extracted\n",
    "            basename = os.path.basename(f)\n",
    "            tmp_xr = xr.open_dataset(tmp_folder + basename)\n",
    "            if gfs: \n",
    "                output_xarray_with_date(tmp_xr, dest_folder, basename, filetype='.zarr', with_date=False, overwrite=overwrite) \n",
    "            else: \n",
    "                output_xarray_with_date(tmp_xr, dest_folder, basename, filetype='.nc', with_date=False, overwrite=overwrite) \n",
    "                #Note the filename is kept as the original from the .zip here. To output the file with a different name, edit the dest_file string parameter.\n",
    "            os.remove(os.path.join('',f)) #remove tmp file\n",
    "\n",
    "        #remove zip file too\n",
    "        os.remove(os.path.join('',tmp_file_path))\n",
    "    else:\n",
    "        if gfs: #load and transfer as zarr\n",
    "            tmp_xr = xr.open_dataset(tmp_file_path)\n",
    "            print(f\"Transferring to GS {zarr_file_path}...\")\n",
    "            tmp_xr.to_zarr(zarr_file_path, mode='w')\n",
    "        else:\n",
    "            tmp_xr = xr.open_dataset(tmp_file_path)\n",
    "            print(f\"Transferring to Destination {file_path}...\")\n",
    "            tmp_xr.to_netcdf(file_path)\n",
    "\n",
    "        os.remove(tmp_file_path)\n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_xarray_with_date(out_xarray, dest_folder: str, dest_filename: str, filetype='.nc', with_date=True, overwrite=False):\n",
    "    \"\"\"\n",
    "    Outputs a file to a specified location and names it according to the date range contained in the xarray\n",
    "    Must have a coordinate dimension named 'time' if outputting with_date=True\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    out_xarray : xarray dataset\n",
    "        The object to output as a netCDF or Zarr file\n",
    "    dest_folder: string\n",
    "        file path for the destination folder\n",
    "    dest_file: string\n",
    "        file name desired for the output data (without specifying the filetype)\n",
    "    filetype: str\n",
    "        Either '.nc' or '.zarr'; specifying the type of output\n",
    "    with_date: boolean\n",
    "        if true, the time range of the xarray (using dimension \"time\") will be appended to the end of file name as '_YYYMM-YYYYMM'\n",
    "    overwrite: boolean\n",
    "        if true, will overwrite the dest_file at the dest_folder location\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import xarray\n",
    "    \n",
    "    #clean up if file name already includes the filetype suffix\n",
    "    dest_filename_new = dest_filename\n",
    "    if dest_filename.strip()[-3:] == '.nc':\n",
    "        dest_filename_new = dest_filename.strip()[:-3]\n",
    "    elif dest_filename.strip()[-5:] == '.zarr':\n",
    "        dest_filename_new = dest_filename.strip()[:-5]\n",
    "    \n",
    "    if with_date:\n",
    "        min_yearmonth = str(out_xarray.time.min().data.astype('datetime64[s]').item().strftime('%Y%m')) #just gets the min date from the xarray in YYYYMM format\n",
    "        max_yearmonth = str(out_xarray.time.max().data.astype('datetime64[s]').item().strftime('%Y%m')) \n",
    "    \n",
    "        processed_filename = (dest_filename_new + '_' + min_yearmonth + '-' + max_yearmonth + filetype)\n",
    "        processed_file_path = os.path.join(dest_folder, processed_filename)\n",
    "        #print(processed_file_path)\n",
    "    else:\n",
    "        processed_filename = dest_filename_new + filetype\n",
    "        processed_file_path = os.path.join(dest_folder, processed_filename)\n",
    "    \n",
    "    #check if file already exists\n",
    "    already_exists = False\n",
    "    if os.path.isfile(processed_file_path): already_exists = True\n",
    "    if dest_folder[0:5] == 'gs://':\n",
    "        import gcsfs\n",
    "        fs = gcsfs.GCSFileSystem()\n",
    "        if fs.exists(processed_file_path):\n",
    "            already_exists = True\n",
    "    \n",
    "    if overwrite or not (already_exists):   #if you want to overwrite or if the file doesnt already exists, then save\n",
    "        if filetype == '.nc':\n",
    "            out_xarray.to_netcdf( processed_file_path )\n",
    "            print(f\"Saved {processed_filename} to {dest_folder}\")\n",
    "        elif filetype == '.zarr':\n",
    "            out_xarray.to_zarr( processed_file_path, mode='w')\n",
    "            print(f\"Saved {processed_filename} to {dest_folder}\")\n",
    "        else: \n",
    "            print(\"Unsupported file output type; please choose '.nc' or '.zarr'\")\n",
    "    else:\n",
    "        print(f\"Cancelling output - {processed_filename} already exists in {dest_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XArray Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xr_open_dataset_custom(file :str, decode_times=True):\n",
    "    \"\"\"\n",
    "    Wrapper function for xarray.open_dataset() but compatible with either .nc (netcdf) or .zarr files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file : string\n",
    "        The file location of an xarray dataset\n",
    "    decode_times : bool, optional\n",
    "        If True, decode times encoded in the standard NetCDF datetime format \n",
    "        into datetime objects. Otherwise, leave them encoded as numbers.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    xr : an xarray dataset object\n",
    "    \"\"\"\n",
    "    #correct if a user specified a .nc but meant .zarr on Google cloud:\n",
    "    updated_file = file\n",
    "    if file.strip()[0:5] == 'gs://' and file.strip()[-3:] == '.nc': \n",
    "        updated_file = file.strip()[0:-3]+file.strip()[-3:].replace('.nc','.zarr')  #hardcoded for netcdfs\n",
    "        \n",
    "    try:\n",
    "        xr_ds = xr.open_dataset(file, decode_times=decode_times) \n",
    "    except:\n",
    "        try:\n",
    "            xr_ds = xr.open_dataset(file, decode_times=decode_times, engine='zarr', chunks={})\n",
    "        except:\n",
    "            print(f'Encountered an error - trying with {updated_file}...')\n",
    "            xr_ds = xr.open_dataset(updated_file, decode_times=decode_times, engine='zarr', chunks={}) \n",
    "            print('Success.')\n",
    "        \n",
    "    return xr_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xr_open_mfdataset_custom(file: str):\n",
    "    \"\"\"\n",
    "    Wrapper function for xarray.open_mfdataset() but compatible with either .netcdf or .zarr files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file : string\n",
    "        The file location of an xarray dataset\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    xr : an xarray dataset object\n",
    "    \"\"\"\n",
    "    #correct if a user specified a .nc but meant .zarr on Google cloud:\n",
    "    updated_file = file\n",
    "    if file.strip()[0:5] == 'gs://' and file.strip()[-3:] == '.nc': \n",
    "        updated_file = file.strip()[0:-3]+file.strip()[-3:].replace('.nc','.zarr')  #hardcoded for netcdfs\n",
    "\n",
    "    try:\n",
    "        xr_ds = xr.open_mfdataset(file) \n",
    "    except:\n",
    "        try:\n",
    "            xr_ds = xr.open_mfdataset(file, engine='zarr', chunks={})  \n",
    "        except:\n",
    "            print(f'Encountered an error - trying with {updated_file}')\n",
    "            xr_ds = xr.open_mfdataset(updated_file, engine='zarr', chunks={})\n",
    "            print('Success.')\n",
    "        \n",
    "    return xr_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function was modified from Luke's\n",
    "def add_time_to_globcolour(file: str):\n",
    "    \"\"\"\n",
    "    Outputs an xarray dataset with a 'time' dimension based on an inputted file\n",
    "    Must have a date in the file name in the YYYYMM format at the end\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file : string\n",
    "        The file location of an xarray dataset that is missing a time dimension.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    ds_tmp : an xarray dataset with the additional time coordinate\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import xarray as xr\n",
    "    import re\n",
    "    \n",
    "    # extract start date\n",
    "    file_month = re.findall(r'\\d{4}\\d{2}', file[-10:])[0]  #just try to find first date in YYYYMM format in last 10 characters of file name\n",
    "    pd_datetime = pd.to_datetime(file_month, format='%Y%m') + np.timedelta64(14, 'D') #add days to be mid-month\n",
    "\n",
    "    # open dataset and create time coordinate and dimension\n",
    "    ds = xr_open_dataset_custom(file) #previously #ds = xr.open_dataset(file)\n",
    "    ds_tmp = ds.assign_coords({'time':pd_datetime}).expand_dims(dim='time', axis=0)\n",
    "    return ds_tmp\n",
    "\n",
    "#chl_test = add_time_to_globcolour(data_folder_root+'CHL/originals/CHL_ARI-ST-GlobColour_L3m-GLOB-100-merged-GSM-CHL1_199802.nc') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_or_0_xr(xr_to_upd, field_name):\n",
    "    \"\"\"\n",
    "    Function to compute the log (base 10) of a DataArray. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Input 1 : DataArray\n",
    "        must have time, ylat, xlon coordinates\n",
    "    Input 2 : String \n",
    "        desired name of the new log field in the output DataArray\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    res : a DataArray of the same shape with log values \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import xarray as xr\n",
    "    from numpy import errstate,isneginf,array\n",
    "\n",
    "    with errstate(divide='ignore'):\n",
    "        n = np.log10(xr_to_upd).values #use .to_numpy() for newer versions of xr\n",
    "    n[np.isneginf(n)]=0\n",
    "    res = xr.DataArray(n, coords={'time': xr_to_upd.time,'ylat': xr_to_upd.ylat,'xlon': xr_to_upd.xlon}, dims=[\"time\", \"ylat\", \"xlon\"], name=field_name)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_least_date_range(xarraylist):\n",
    "    \"\"\"\n",
    "    Function to compute the minimum overlapping time range of a set of xarray datasets \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Input 1 : xarraylist\n",
    "        A list of xarray datasets. Each must contain a 'time' coordinate\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    min_date : string \n",
    "        The most recent start date (in YYYY-MM format) among the datasets \n",
    "    max_date : string\n",
    "        The earliest end date (in YYYY-MM format) among the datasets\n",
    "    \"\"\"\n",
    "    import xarray as xr\n",
    "    \n",
    "    min_date = []\n",
    "    max_date = []\n",
    "    for f in xarraylist:\n",
    "        min_date.append(f.time.min().data.astype('datetime64[s]').item())\n",
    "        max_date.append(f.time.max().data.astype('datetime64[s]').item())\n",
    "    \n",
    "    return max(min_date).strftime('%Y-%m'), min(max_date).strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fCO2_to_pCO2\n",
    "\n",
    "These functions were taken from the fCO2_to_pCO2.ipynb file with no changes other than variable naming updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitError(Exception):\n",
    "    pass\n",
    "\n",
    "def check_array_bounds(arr, lims, action=\"warn\", name=\"\"):\n",
    "    \"\"\"\n",
    "    Checks that units are within the given limits. If not, then\n",
    "    will raise/warn the user. Will always raise an error if more\n",
    "    than half of the non-nan values are outside the limits.\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : array-like\n",
    "        The array that will be checked\n",
    "    lims : tuple\n",
    "        lower and upper limits of checks\n",
    "        note that limits are exclusive (i.e. < and >, and not >=/<=)\n",
    "    action: string\n",
    "        raise - will raise an error and not continue\n",
    "        warn - will throw a warning and mask values with nan\n",
    "        quiet - same as warn, but without warning\n",
    "        ignore - nothing will be done, but may result in bad data\n",
    "    name: string\n",
    "        if given, will inform the user of the name of the array\n",
    "        to make debugging easier\n",
    "    Return\n",
    "    ------\n",
    "    arr : array-like\n",
    "        returns the array, but if warn or quiet, will be masked\n",
    "        with nans\n",
    "    \"\"\"\n",
    "\n",
    "    from numpy import array, any, nan, isnan\n",
    "    from warnings import warn\n",
    "\n",
    "    arr = array(arr, ndmin=1, dtype=float)\n",
    "    if arr.size <= 2:\n",
    "        return arr\n",
    "\n",
    "    outside = (arr < lims[0]) | (arr > lims[1])\n",
    "\n",
    "    non_nan_count = arr.size - isnan(arr).sum()\n",
    "    half_outside = outside.sum() > (non_nan_count * 0.5)\n",
    "    if half_outside:\n",
    "        raise UnitError(\n",
    "            f\"More than half of the values in {name} are outside the limits \"\n",
    "            f\"{str(lims)}. Check that input contains the correct units.\"\n",
    "        )\n",
    "\n",
    "    msg = (\n",
    "        f\"There are {outside.sum():d} values that do not fall within \"\n",
    "        f\"the given limits {str(lims)}\"\n",
    "        f\" of {name}\"\n",
    "        if name != \"\"\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    if any(outside) & (action == \"raise\"):\n",
    "        raise UnitError(msg)\n",
    "    elif action == \"warn\":\n",
    "        if any(outside):\n",
    "            warn(msg, Warning)\n",
    "        arr[outside] = nan\n",
    "    elif action == \"quiet\":\n",
    "        arr[outside] = nan\n",
    "    elif action == \"ignore\":\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception(\"action must have raise/warn/quiet/ignore as inputs\")\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_K(temp_K):\n",
    "    return check_array_bounds(\n",
    "        arr=temp_K, lims=(270, 318.5), action=\"warn\", name=\"temperature (K)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pres_atm(pres_atm):\n",
    "    return check_array_bounds(\n",
    "        arr=pres_atm, lims=(0.5, 1.5), action=\"warn\", name=\"Pressure (atm)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CO2_mol(CO2_mol):\n",
    "    return check_array_bounds(\n",
    "        arr=CO2_mol,\n",
    "        lims=(5e-6, 0.08),\n",
    "        action=\"warn\",\n",
    "        name=\"CO2 mole fraction (ppm)\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_correction(temp_in, temp_out):\n",
    "    \"\"\"\n",
    "    Calculate a correction factor for the temperature difference between the\n",
    "    intake and equilibrator. This is based on the empirical relationship used\n",
    "    in Takahashi et al. 1993.\n",
    "    pCO2_Tout = pCO2_Tin * T_factor\n",
    "    Parameters\n",
    "    ----------\n",
    "    temp_in : np.array\n",
    "        temperature at which original pCO2 is measured\n",
    "    temp_out : np.array\n",
    "        temperature for which pCO2 should be represented\n",
    "    Return\n",
    "    ------\n",
    "    factor : np.array\n",
    "        a correction factor to be multiplied to pCO2 (unitless)\n",
    "    References\n",
    "    ----------\n",
    "    Takahashi, Taro et al. (1993). Seasonal variation of CO2 and nutrients in\n",
    "        the high-latitude surface oceans: A comparative study. Global\n",
    "        Biogeochemical Cycles, 7(4), 843–878. https://doi.org/10.1029/93GB02263\n",
    "    \"\"\"\n",
    "\n",
    "    from numpy import array, exp\n",
    "\n",
    "    # see the Takahashi 1993 paper for full description\n",
    "\n",
    "    Ti = array(temp_in)\n",
    "    To = array(temp_out)\n",
    "\n",
    "    factor = exp(0.0433 * (To - Ti) - 4.35e-05 * (To ** 2 - Ti ** 2))\n",
    "\n",
    "    return factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def virial_coeff(temp_K1, pres_atm1, xCO2_mol=None):\n",
    "    \"\"\"\n",
    "    Calculate the ideal gas correction factor for converting pCO2 to fCO2.\n",
    "    fCO2 = pCO2 * virial_expansion\n",
    "    pCO2 = fCO2 / virial_expansion\n",
    "    Based on the Lewis and Wallace 1998 Correction.\n",
    "    Parameters\n",
    "    ----------\n",
    "    press_atm : np.array\n",
    "        uncorrected pressure in atm\n",
    "    temp_K : np.array\n",
    "        temperature in degrees Kelvin\n",
    "    xCO2_mol : np.array\n",
    "        mole fraction of CO2. Can be pCO2/fCO2 if xCO2 is not defined or can\n",
    "        leave this as undefined as makes only a small impact on output\n",
    "    Return\n",
    "    ------\n",
    "    virial_expression : np.array\n",
    "        the factor to multiply with pCO2. Unitless\n",
    "    Examples\n",
    "    --------\n",
    "    The example below is from Dickson et al. (2007)\n",
    "    >>> 350 * virial_coeff(298.15, 1)  # CO2 [uatm] * correction factor\n",
    "    348.8836492182758\n",
    "    References\n",
    "    ----------\n",
    "    Weiss, R. (1974). Carbon dioxide in water and seawater: the solubility of a\n",
    "        non-ideal gas. Marine Chemistry, 2(3), 203–215.\n",
    "        https://doi.org/10.1016/0304-4203(74)90015-2\n",
    "    Compared with the Seacarb package in R\n",
    "    \"\"\"\n",
    "    from numpy import array, exp\n",
    "    #import check_units as check\n",
    "\n",
    "    T = temp_K(temp_K1)\n",
    "    P = pres_atm(pres_atm1)\n",
    "    C = array(xCO2_mol)\n",
    "    R = 82.057  # gas constant for ATM\n",
    "\n",
    "    temp_K(T)\n",
    "    pres_atm(P)\n",
    "\n",
    "    # B is the virial coefficient for pure CO2\n",
    "    B = -1636.75 + 12.0408 * T - 0.0327957 * T ** 2 + 3.16528e-5 * T ** 3\n",
    "    # d is the virial coefficient for CO2 in air\n",
    "    d = 57.7 - 0.118 * T\n",
    "\n",
    "    # \"x2\" term often neglected (assumed = 1) in applications of Weiss's\n",
    "    # (1974) equation 9\n",
    "    if xCO2_mol is not None:\n",
    "        CO2_mol(C)\n",
    "        x2 = (1 - C) ** 2\n",
    "    else:\n",
    "        x2 = 1\n",
    "\n",
    "    ve = exp(P * (B + 2 * x2 * d) / (R * T))\n",
    "\n",
    "    return ve\n",
    "\n",
    "#350 * virial_coeff(298.15, 1) #348.88364922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is not required in this set of code but preserving for other use\n",
    "def fCO2_to_pCO2(fCO2SW_uatm, tempSW_C, pres_hPa=1013.25, tempEQ_C=None):\n",
    "    \"\"\"\n",
    "    Convert fCO2 to pCO2 for SOCAT in sea water. A simple version of the\n",
    "    equation would simply be:\n",
    "        pCO2sw = fCO2sw / virial_exp\n",
    "    where the virial expansion is calculated without xCO2\n",
    "    We get a simple approximate for equilibrator xCO2 with:\n",
    "        xCO2eq = fCO2sw * deltaTemp(sw - eq) / press_eq\n",
    "    pCO2sw is then calculated with:\n",
    "        pCO2sw = fCO2sw / virial_exp(xCO2eq)\n",
    "    Parameters\n",
    "    ----------\n",
    "    fCO2SW_uatm : array\n",
    "        seawater fugacity of CO2 in micro atmospheres\n",
    "    tempSW_C : array\n",
    "        sea water temperature in degrees C\n",
    "    pres_hPa : array\n",
    "        equilibrator pressure in hecto Pascals\n",
    "    tempEQ_C : array\n",
    "        equilibrator temperature in degrees C\n",
    "    Returns\n",
    "    -------\n",
    "    pCO2SW_uatm : array\n",
    "        partial pressure of CO2 in seawater\n",
    "    Note\n",
    "    ----\n",
    "    In FluxEngine, they account fully solve for the original xCO2 that is used\n",
    "    in the calculation of the virial exponent. I use the first estimate of\n",
    "    xCO2 (based on fCO2 rather than pCO2). The difference between the two\n",
    "    approaches is so small that it is not significant to be concerned. Their\n",
    "    correction is more precise, but the difference between their iterative\n",
    "    correction and our approximation is on the order of 1e-14 atm (1e-8 uatm).\n",
    "    Examples\n",
    "    --------\n",
    "    >>> fCO2_to_pCO2(380, 8)\n",
    "    381.50806485658234\n",
    "    >>> fCO2_to_pCO2(380, 8, pres_hPa=985)\n",
    "    381.4659553134281\n",
    "    >>> fCO2_to_pCO2(380, 8, pres_hPa=985, tempEQ_C=14)\n",
    "    381.466027968504\n",
    "    \"\"\"\n",
    "    #import check_units as check\n",
    "    #import auxiliary_equations as eqs\n",
    "\n",
    "    # if equilibrator inputs are None, tempEQ=tempSW\n",
    "    if tempEQ_C is None:\n",
    "        tempEQ_was_None = True\n",
    "        tempEQ_C = tempSW_C\n",
    "    else:\n",
    "        tempEQ_was_None = False\n",
    "\n",
    "    # standardise the inputs and convert units\n",
    "    fCO2sw = CO2_mol(fCO2SW_uatm * 1e-6)\n",
    "    Tsw = temp_K(tempSW_C + 273.15)\n",
    "    Teq = temp_K(tempEQ_C + 273.15)\n",
    "    Peq = pres_atm(pres_hPa / 1013.25)\n",
    "\n",
    "    # calculate the CO2 diff due to equilibrator and seawater temperatures\n",
    "    # if statement is there to save a bit of time\n",
    "    if tempEQ_was_None:\n",
    "        dT = 1.0\n",
    "    else:\n",
    "        dT = temperature_correction(Tsw, Teq)\n",
    "\n",
    "    # a best estimate of xCO2 - this is an aproximation\n",
    "    # one would have to use pCO2 / Peq to get real xCO2\n",
    "    # Not getting the exact equilibrator xCO2\n",
    "    xCO2eq = fCO2sw * dT / Peq\n",
    "\n",
    "    pCO2SW = fCO2sw / virial_coeff(Tsw, Peq, xCO2eq)\n",
    "    pCO2SW_uatm = pCO2SW * 1e6\n",
    "\n",
    "    return pCO2SW_uatm\n",
    "\n",
    "#fCO2_to_pCO2(380, 8) #381.50806486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is not required in this set of code but preserving for other use\n",
    "def pCO2_to_fCO2(pCO2SW_uatm, tempSW_C, pres_hPa=None, tempEQ_C=None):\n",
    "    \"\"\"\n",
    "    Convert fCO2 to pCO2 for SOCAT in sea water. A simple version of the\n",
    "    equation would simply be:\n",
    "        fCO2sw = pCO2sw / virial_exp\n",
    "    where the virial expansion is calculated without xCO2\n",
    "    We get a simple approximate for equilibrator xCO2 with:\n",
    "        xCO2eq = pCO2sw * deltaTemp(sw - eq) / press_eq\n",
    "    fCO2sw is then calculated with:\n",
    "        fCO2sw = pCO2sw * virial_exp(xCO2eq)\n",
    "    Parameters\n",
    "    ----------\n",
    "    pCO2SW_uatm : array\n",
    "        seawater fugacity of CO2 in micro atmospheres\n",
    "    tempSW_C : array\n",
    "        sea water temperature in degrees C/K\n",
    "    tempEQ_C : array\n",
    "        equilibrator temperature in degrees C/K\n",
    "    pres_hPa : array\n",
    "        pressure in kilo Pascals\n",
    "    Returns\n",
    "    -------\n",
    "    fCO2SW_uatm : array\n",
    "        partial pressure of CO2 in seawater\n",
    "    Note\n",
    "    ----\n",
    "    In FluxEngine, they account for the change in xCO2. This error is so small\n",
    "    that it is not significant to be concerned about it. Their correction is\n",
    "    more precise, but the difference between their iterative correction and our\n",
    "    approximation is less than 1e-14 atm (or 1e-8 uatm).\n",
    "    Examples\n",
    "    --------\n",
    "    >>> pCO2_to_fCO2(380, 8)\n",
    "    378.49789637942064\n",
    "    >>> pCO2_to_fCO2(380, 8, pres_hPa=985)\n",
    "    378.53967828231225\n",
    "    >>> pCO2_to_fCO2(380, 8, pres_hPa=985, tempEQ_C=14)\n",
    "    378.53960618459695\n",
    "    \"\"\"\n",
    "    #import check_units as check\n",
    "    #import auxiliary_equations as eqs\n",
    "\n",
    "    # if equilibrator inputs are None then make defaults Patm=1, tempEQ=tempSW\n",
    "    if tempEQ_C is None:\n",
    "        tempEQ_C = tempSW_C\n",
    "    if pres_hPa is None:\n",
    "        pres_hPa = 1013.25\n",
    "\n",
    "    # standardise the inputs and convert units\n",
    "    pCO2sw = CO2_mol(pCO2SW_uatm * 1e-6)\n",
    "    Tsw = temp_K(tempSW_C + 273.15)\n",
    "    Teq = temp_K(tempEQ_C + 273.15)\n",
    "    Peq = pres_atm(pres_hPa / 1013.25)\n",
    "\n",
    "    # calculate the CO2 diff due to equilibrator and seawater temperatures\n",
    "    dT = temperature_correction(Tsw, Teq)\n",
    "    # a best estimate of xCO2 - this is an aproximation\n",
    "    # one would have to use pCO2 / Peq to get real xCO2\n",
    "    xCO2eq = pCO2sw * dT / Peq\n",
    "\n",
    "    fCO2sw = pCO2sw * virial_coeff(Tsw, Peq, xCO2eq)\n",
    "    fCO2sw_uatm = fCO2sw * 1e6\n",
    "\n",
    "    return fCO2sw_uatm\n",
    "\n",
    "#pCO2_to_fCO2(380, 8) #378.49789638"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pco2_from_xco2(xco2,pres,T,S):\n",
    "    \"\"\"\n",
    "    Function taken directly from the Mauna_ESRL.ipynb script. Used in processing atmospheric CO2 data for HPD product\n",
    "    \"\"\"\n",
    "    import math\n",
    "    #test:\n",
    "    #% xco2=378.16;\n",
    "    #% pres=1018.02;\n",
    "    #% T=15;\n",
    "    #% S=35;\n",
    "\n",
    "    #calculate pressure at 100% humidity\n",
    "    a1 = -7.85951783\n",
    "    a2 = 1.84408259\n",
    "    a4 = 22.6807411\n",
    "    a5 = -15.9618719\n",
    "    a3 = -11.7866497\n",
    "    a6 = 1.80122502\n",
    "    \n",
    "    Tc = 647.096;\n",
    "    pc = 22064000;\n",
    "    T=T+273.15;\n",
    "\n",
    "    Phi=(1-(T/Tc));\n",
    "    lnform=(Tc/T)*(a1*Phi+a2*Phi**(1.5)+a3*Phi**3+a4*Phi**(3.5)+a5*Phi**(4)+a6*Phi**(7.5));\n",
    "    Psig = 2.71828**(lnform)*pc\n",
    "\n",
    "    molarity=(31.998*S)/(10**3-1.005*S);\n",
    "    Psi=0.90799-0.08992*(0.5*molarity)+0.18458*(0.5*molarity)**2-0.07395*(0.5*molarity)**3-0.00221*(0.5*molarity)**4;\n",
    "    Psigcor = Psig*2.71828**(-0.018*Psi*molarity)\n",
    "\n",
    "    # convert pressure from hPa to Pascal\n",
    "    pres=pres*100;\n",
    "\n",
    "    # calculate pCO2 wet in Pa\n",
    "    pco2_Pa= (xco2*10**-6)*(pres-Psigcor);\n",
    "\n",
    "    # and convert into muatm\n",
    "    pco2=pco2_Pa*9.8692326671601E-6*10**6;\n",
    "\n",
    "    return pco2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(y, pred):\n",
    "    \"\"\"\n",
    "    Create metrics for evaluation of a model's predictions\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : numpy array\n",
    "        actual values for a dependent variable\n",
    "    pred : numpy array\n",
    "        predicted values for the dependent variable\n",
    "    Returns\n",
    "    -------\n",
    "    scores : dictionary\n",
    "        a dictionary of 13 metrics \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import r2_score, max_error, mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "    \n",
    "    y_mean = np.mean(y)\n",
    "    pred_mean = np.mean(pred)\n",
    "    centered_rmse = np.sqrt(np.square((pred - pred_mean) - (y - y_mean)).sum()/pred.size)\n",
    "\n",
    "    scores = {\n",
    "        'mse':mean_squared_error(y, pred),\n",
    "        'mae':mean_absolute_error(y, pred),\n",
    "        'medae':median_absolute_error(y, pred),\n",
    "        'max_error':max_error(y, pred),\n",
    "        'bias':pred.mean() - y.mean(),\n",
    "        'r2':r2_score(y, pred),\n",
    "        'corr':np.corrcoef(y,pred)[0,1],\n",
    "        'cent_rmse':centered_rmse,\n",
    "        'stdev' :np.std(pred),\n",
    "        'amp_ratio':(np.max(pred)-np.min(pred))/(np.max(y)-np.min(y)), # added when doing temporal decomposition\n",
    "        'stdev_ref':np.std(y),\n",
    "        'range_ref':np.max(y)-np.min(y),\n",
    "        'iqr_ref':np.subtract(*np.percentile(y, [75, 25]))\n",
    "        }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is useful to format the output from evaluate_test()\n",
    "def print_dict_as_table(seq, columns=4):\n",
    "    \"\"\"\n",
    "    Prints a dictionary formatted as a table\n",
    "    Parameters\n",
    "    ----------\n",
    "    seq : dictionary\n",
    "        a dictionary to print\n",
    "    columns : int\n",
    "        number of columns to print\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    table = ''\n",
    "    col_height = (len(seq) // columns) +1\n",
    "    for x in range(col_height):\n",
    "        for col in range(columns):\n",
    "            if x + (col_height * col) <= len(seq)-1:\n",
    "                a = list(seq.keys())[x + (col_height * col)]\n",
    "                b = seq[list(seq.keys())[x + (col_height * col)]]\n",
    "                num = '{:.9s}: {:.3f}'.format(a + ' '*20, round(b,3))\n",
    "            else:\n",
    "                num = ''\n",
    "            table += ('%s' % (num)).ljust(24)\n",
    "        table += '\\n'\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Functions\n",
    "\n",
    "TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
